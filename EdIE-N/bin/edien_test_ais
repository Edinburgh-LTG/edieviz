#!/usr/bin/env python
import re
import os
import copy
import torch
import dataclasses
from argparse import ArgumentParser
from xml.etree import ElementTree as ET
from xml.dom import minidom
from mlconf import Blueprint
from edien import EdIENPath
from edien.data.ess import EdIELoader, EdIEDoc
from edien.train_utils import only_pseudorandomness_please, eval_loop


def prettify(elem):
    """Return a pretty-printed XML string for the Element.
    """
    rough_string = ET.tostring(elem, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent=" ")


def to_xml(doc,
           serialize_labels=False,
           serialize_ents=False,
           serialize_relations=False,
           **kwargs):

    xml_copy = copy.deepcopy(doc.xml_parse)

    root = xml_copy.getroot()

    standoffs = xml_copy.findall(EdIEDoc.XML_STANDOFF)
    assert len(standoffs) == 1
    standoff = standoffs[0]

    # if we serialize ents - we assume they were predicted
    if serialize_ents:
        # Create new ents entry
        ents = ET.SubElement(standoff, EdIEDoc.XML_ST_ENTS)
        ents.set('source', 'edien')

    if serialize_relations:
        rels = ET.SubElement(standoff, EdIEDoc.XML_ST_RELS)
        rels.set('source', 'scispacy')

    if serialize_labels:
        for label in doc.labels:
            ent = ET.SubElement(ents, EdIEDoc.XML_ENTITY)
            ent_type = '%s%s' % (EdIEDoc.XML_LABEL, label)
            ent.set(EdIEDoc.XML_ENTITY_TYPE, ent_type)

    # If we ask to serialize ents - we assume we predicted them
    if serialize_ents:
        for sent in doc.sentences:
            for mod in sent.get_entities(sent.mod_tags):

                ent_xml = mod.to_xml()
                ents.append(ent_xml)

            for find in sent.get_entities(sent.ner_tags):

                ent_xml = find.to_xml()
                ents.append(ent_xml)

    if serialize_relations:

        for rel in doc.relations:

            rel_xml = rel.to_xml()
            rels.append(rel_xml)

    return root


def to_xml_string(doc, **kwargs):
    doc = to_xml(doc, **kwargs)
    xml_str = prettify(doc)
    xml_str = re.sub(r'\n\s+\n', '\n', xml_str).rstrip()
    return xml_str


def make_neg_predict_consistent(negs, ents):
    """Propagate decision of B- token to following I- tokens"""
    assert len(negs) == len(ents)
    neg_decision = None
    for i in range(len(negs)):
        assert negs[i] is not None
        if ents[i][:2] == 'B-':
            neg_decision = negs[i]
        elif ents[i][:2] == 'I-':
            if neg_decision is not None:
                negs[i] = neg_decision
        else:
            neg_decision = None
    return negs, ents


def write_conll(sentences, y, preds, bp):

    targets = bp.data.targets
    subfolder = bp.data.dataset.foldername
    folder_path = bp.paths.for_output(subfolder)

    if not os.path.isdir(folder_path):
        print('Creating directory %s' % folder_path)
        os.makedirs(folder_path)

    for target in targets:
        if getattr(preds, target, None) is None:
            print("==Warning==: Skipping %s as wasn't predicted\n" % target)
            continue
        filename = '%s.conll' % target
        out_path = os.path.join(folder_path, filename)
        i = 0
        chunks = []
        for s in sentences:
            sent_len = len(s.tokens)
            # TODO: for cases where we do sentence prediction
            # need to refactor this part of the code
            sent_ner_preds = getattr(preds, target)[i:i + sent_len]
            conll_line = s.to_conll(sent_ner_preds, target)
            chunks.append('%s\n' % conll_line)
            i += sent_len
        print('Writing conll output to %s' % out_path)
        with open(out_path, 'w') as f:
            f.writelines(chunks)

        if target != 'negation' and 'negation' in targets:
            filename = 'negation_%s.conll' % target
            out_path = os.path.join(folder_path, filename)
            i = 0
            chunks = []
            for s in sentences:
                sent_len = len(s.tokens)
                # Quickly change to evaluate neg with conll scoring
                # We use gold entities - only evaluating negation here
                sent_ner_gold = getattr(s, target)
                sent_neg_gold = getattr(s, 'negation')
                sent_ner_preds = list(getattr(preds, target)[i:i + sent_len])
                sent_neg_preds = list(getattr(preds, 'negation')[i:i + sent_len])

                sent_neg_preds, sent_ner_preds = make_neg_predict_consistent(sent_neg_preds, sent_ner_preds)

                comb_preds = [('%sneg_%s' % (ner[:2], ner[2:])) if (neg == 'neg' and ner != 'O') else ner
                              for neg, ner in zip(sent_neg_preds, sent_ner_preds)]
                comb_gold = [('%sneg_%s' % (ner[:2], ner[2:])) if (neg == 'neg' and ner != 'O') else ner
                             for neg, ner in zip(sent_neg_gold, sent_ner_gold)]
                lines = []
                for fields in zip(s.tokens, s.pos_tags, comb_gold, comb_preds):
                    lines.append('%s\n' % ' '.join(fields))
                chunk = ''.join(lines)
                chunk = '%s\n' % chunk
                chunks.append(chunk)
                i += sent_len
            print('Writing conll output to %s' % out_path)
            with open(out_path, 'w') as f:
                f.writelines(chunks)


if __name__ == "__main__":

    parser = ArgumentParser(description='Dependency parser trainer')

    parser.add_argument('--experiment', type=str, required=True,
                        help='Path to folder of experiment')
    parser.add_argument('--in_folder', required=True,
                        help='Path to the ais ann.xml files')
    # parser.add_argument('--visualise', action='store_true',
    #                     help='Whether to visualise training or not.')
    parser.add_argument('--verbose', action='store_true',
                        help='Whether to print additional info such '
                        'as model and vocabulary info.')

    args = parser.parse_args()

    blueprint_file = os.path.join(args.experiment, EdIENPath.BP_FILE)
    print(blueprint_file)
    if not os.path.isfile(blueprint_file):
        raise ValueError("Couldn't find blueprint file %s" % blueprint_file)
    conf = Blueprint.from_file(blueprint_file)
    print('Successfully loaded blueprint from %s' % blueprint_file)

    only_pseudorandomness_please(conf.seed)

    conf.paths.experiment_name = conf.name
    # Make sure we aren't trying to create vocab on dev
    conf.data.vocab_encoder.load_if_exists = True

    # Creates model with initialised params
    bp = conf.build()

    # Replaces model parameters with those loaded from saved model
    model_path = bp.paths.model_path
    print('Loading model from %s' % model_path)
    model = bp.model.load(model_path)

    if conf.device.startswith('cuda'):
        # Set default gpu
        model.to(conf.device)

    print('Evaluating on %s..' % args.in_folder)
    loaded = EdIELoader.load(args.in_folder,
                             no_labels=True)
    test_sents = loaded.sentences
    test = bp.data.encode(test_sents, bp.paths)
    # data_folder = bp.data.dataset.test_filename
    base_foldername = os.path.basename(os.path.normpath(args.in_folder))

    for task in bp.model.model.tasks:
        model.model.tasks[task].label_vocab = bp.data.vocab_encoder.vocabs[task]

    print(base_foldername)
    folder_path = bp.paths.for_output(base_foldername)
    print(folder_path)

    if not os.path.isdir(folder_path):
        print('Creating directory %s' % folder_path)
        os.makedirs(folder_path)

    preds = model.predict(test)

    dec_test_y = bp.data.decode(test)
    dec_preds = bp.data.decode(preds)

    preds_by_sent_id = dict()
    i = 0
    for sent in test_sents:
        sent_len = len(sent)
        sent_preds = dict()
        for target in dec_preds._fields:
            p = getattr(dec_preds, target)[i:i + sent_len]
            sent_preds[target] = p
        preds_by_sent_id[sent.sent_id] = sent_preds
        i += sent_len

    for doc in loaded.docs:
        doc_sents = []
        for sent in doc.sentences:
            preds_sid = preds_by_sent_id.get(sent.sent_id, None)
            if preds_sid is not None:
                new_sent = dataclasses.replace(sent, **preds_sid)
                doc_sents.append(new_sent)
            else:
                doc_sents.append(sent)
        new_doc = dataclasses.replace(doc, sentences=doc_sents)
        xml_str = to_xml_string(new_doc, serialize_ents=True)
        outfile = '%s.edien.xml' % doc.doc_id
        outpath = os.path.join(folder_path, outfile)
        with open(outpath, 'w') as f:
            f.write(xml_str)
    print('Wrote %d files to %s' % (len(loaded.docs), folder_path))
